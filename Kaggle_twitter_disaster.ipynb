{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_twitter_disaster.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkWfqa9lBXWd/9I5l/S1Yg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryabhatt-O/Text-Processing/blob/main/Kaggle_twitter_disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.callbacks import EarlyStopping\n",
        "from nltk import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "# stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "Yw1A4ea3Mzpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
        "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
        "    :param actual: Array containing the actual target classes\n",
        "    :param predicted: Matrix with class predictions, one probability per class\n",
        "    \"\"\"\n",
        "    # Convert 'actual' to a binary array if it's not already:\n",
        "    if len(actual.shape) == 1:\n",
        "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
        "        for i, val in enumerate(actual):\n",
        "            actual2[i, val] = 1\n",
        "        actual = actual2\n",
        "\n",
        "    clip = np.clip(predicted, eps, 1 - eps)\n",
        "    rows = actual.shape[0]\n",
        "    vsota = np.sum(actual * np.log(clip))\n",
        "    return -1.0 / rows * vsota"
      ],
      "metadata": {
        "id": "2Gq690cdN_ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8p0qMExvvnV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(io.StringIO(uploaded['train.csv'].decode(\"utf-8\")))\n"
      ],
      "metadata": {
        "id": "7gDeNI2cv7hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "nGEGF9YWO9qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_data.target.values\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(train_data.text.values, y, \n",
        "                                                  stratify=y, \n",
        "                                                  random_state=42, \n",
        "                                                  test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "eeMyUfdsOicw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "FVbDNdBoNV4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always start with these features. They work (almost) everytime!\n",
        "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "\n",
        "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
        "tfv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_tfv =  tfv.transform(xtrain) \n",
        "xvalid_tfv = tfv.transform(xvalid)"
      ],
      "metadata": {
        "id": "XjGJ2GBqwYrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(C=1.0)\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "NXdUeHkBPe7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), stop_words = 'english')\n",
        "\n",
        "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
        "ctv.fit(list(xtrain) + list(xvalid))\n",
        "xtrain_ctv =  ctv.transform(xtrain) \n",
        "xvalid_ctv = ctv.transform(xvalid)"
      ],
      "metadata": {
        "id": "sCNTPgF4PnQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = xtrain_tfv.T"
      ],
      "metadata": {
        "id": "XPGd4TvUi0iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "svd = decomposition.TruncatedSVD(n_components=120)\n",
        "svd.fit(A)\n",
        "# u, s, v = np.linalg.svd(A, full_matrices=True)"
      ],
      "metadata": {
        "id": "BhHZC-EJi0xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lr = LogisticRegression(C = 1.0, penalty ='l2')\n",
        "clf_lr.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf_lr.predict_proba(xvalid_ctv)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "ZU7QSatvPrqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
        "RIDGE_MODEL = RidgeClassifier(alpha=0.005994842503189409, random_state=13)\n",
        "\n",
        "\n",
        "RIDGE_MODEL.fit(xtrain_ctv, ytrain)\n",
        "# predictions = RIDGE_MODEL.predict_proba(xvalid_ctv)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "gv68Z1K0pELx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB(alpha=2.782559402207126)\n",
        "clf.fit(xtrain_tfv, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_tfv)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "DgnExNT1P2Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
        "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
      ],
      "metadata": {
        "id": "mWYmUddCNxTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid search cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
        "logreg=LogisticRegression()\n",
        "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
        "logreg_cv.fit(xtrain_ctv, ytrain)\n",
        "\n",
        "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
        "print(\"accuracy :\",logreg_cv.best_score_)"
      ],
      "metadata": {
        "id": "k_jW2_RYOC0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lr = LogisticRegression(C = 1.0, penalty ='l2')\n",
        "clf_lr.fit(xtrain_ctv, ytrain)\n",
        "predictions = clf_lr.predict_proba(xvalid_ctv)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "DbTuJm6oOClu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = decomposition.TruncatedSVD(n_components=120)\n",
        "svd.fit(xtrain_ctv)\n",
        "xtrain_svd = svd.transform(xtrain_ctv)\n",
        "xvalid_svd = svd.transform(xvalid_ctv)\n",
        "\n",
        "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
        "scl = preprocessing.StandardScaler()\n",
        "scl.fit(xtrain_svd)\n",
        "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
        "xvalid_svd_scl = scl.transform(xvalid_svd)"
      ],
      "metadata": {
        "id": "B93j9YLrbmO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "clf.fit(xtrain_svd_scl, ytrain)\n",
        "predictions = clf.predict_proba(xvalid_svd_scl)\n",
        "\n",
        "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
      ],
      "metadata": {
        "id": "wLDW_VhtbqE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "0byoDJu-0KwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stopwords with nltk.\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
        "train_data['Text_without_stopwords'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
      ],
      "metadata": {
        "id": "X41QMR1-0VxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Text_stemmed_without_no'] = train_data['Text_without_stopwords'].str.replace('\\d+', '')"
      ],
      "metadata": {
        "id": "iEqRvCHa0cO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[\"Text_stemmed_without__stopwords_number_punc\"] = train_data['Text_stemmed_without_no'].str.replace('[^\\w\\s]','')"
      ],
      "metadata": {
        "id": "kSpsYbtX02IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.drop(['text', 'Text_without_stopwords','Text_stemmed_without_no',], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "4uxvG2jb1uro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "aRGAYZZI2bwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Text'] = train_data['Text_stemmed_without__stopwords_number_punc'].apply(lambda x: [sno.stem(x)]) "
      ],
      "metadata": {
        "id": "FnWBLN2L2sYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data.drop('Text_stemmed_without__stopwords_number_punc',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "N_p86iVc3FCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Final_text']=[\" \".join(review) for review in train_data['Text'].values]\n",
        "train_data.drop('Text',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "P6Yw8dr43otU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.drop('keyword',axis=1, inplace=True)\n",
        "train_data.drop('location',axis=1, inplace=True)\n",
        "train_data"
      ],
      "metadata": {
        "id": "DmlTkMlv7bEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n"
      ],
      "metadata": {
        "id": "Fal-WY-Oekfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
        "\n",
        "## let's get counts for the  tweets in the data\n",
        "train_vectors = count_vectorizer.fit_transform(train_data[\"Final_text\"])"
      ],
      "metadata": {
        "id": "kSbAZS4jeqOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pq1lU19PrhJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(train_vectors,train_data[\"target\"])"
      ],
      "metadata": {
        "id": "UuWph2GFZEvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf_lr=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')"
      ],
      "metadata": {
        "id": "J1arUhC1piUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "scores = model_selection.cross_val_score(clf, train_vectors, train_data[\"target\"], cv=3, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "id": "wuNoI-1ydKHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "v35Q38spZ81j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv(io.StringIO(uploaded['test.csv'].decode(\"utf-8\")))"
      ],
      "metadata": {
        "id": "lv5cyoM3Z8w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctv_test = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), stop_words = 'english')\n",
        "\n",
        "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
        "ctv_test.fit(list(test_data.text.values))\n",
        "ctV_test =  ctv.transform(test_data.text.values) \n",
        "# xvalid_ctv = ctv.transform(xvalid)"
      ],
      "metadata": {
        "id": "vDfLtGQzQoZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_3rd_attempt = RIDGE_MODEL.predict(ctV_test)"
      ],
      "metadata": {
        "id": "uf5XJwHTQoTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xlk2hey8QoPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YyaRZlSeQoKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r3Jl45ZfQoFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stopwords with nltk.\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
        "test_data['Text_without_stopwords'] = test_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
      ],
      "metadata": {
        "id": "Uxpj1OM1iLsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Text_stemmed_without_no'] = test_data['Text_without_stopwords'].str.replace('\\d+', '')"
      ],
      "metadata": {
        "id": "yADdUWcKie2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[\"Text_stemmed_without__stopwords_number_punc\"] = test_data['Text_stemmed_without_no'].str.replace('[^\\w\\s]','')"
      ],
      "metadata": {
        "id": "ruiARPNSil7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop(['text', 'Text_without_stopwords','Text_stemmed_without_no',], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gKB3psQaisek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "Uz5RQV0fivxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Text'] = test_data['Text_stemmed_without__stopwords_number_punc'].apply(lambda x: [sno.stem(x)]) "
      ],
      "metadata": {
        "id": "ihDA2m67iv2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop('Text_stemmed_without__stopwords_number_punc',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "PtsxwC3yiv6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Final_text']=[\" \".join(review) for review in test_data['Text'].values]\n",
        "test_data.drop('Text',axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "uwB-S236iv_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop('keyword',axis=1, inplace=True)\n",
        "test_data.drop('location',axis=1, inplace=True)\n",
        "test_data"
      ],
      "metadata": {
        "id": "e-NlzuzTiwC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vectors = count_vectorizer.transform(test_data[\"Final_text\"])"
      ],
      "metadata": {
        "id": "4hwMmrCciwHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = clf.predict(test_vectors)\n"
      ],
      "metadata": {
        "id": "7V03lLODjmZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "OGUalRl2kkZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission  = pd.read_csv(io.StringIO(uploaded['submission.csv'].decode(\"utf-8\")))"
      ],
      "metadata": {
        "id": "DdEBl95tkkyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission[\"target\"] = predicted_3rd_attempt"
      ],
      "metadata": {
        "id": "O9Bc01gHkk2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.head()"
      ],
      "metadata": {
        "id": "-BtGACYfkk7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv(\"submission3.csv\", index=False)"
      ],
      "metadata": {
        "id": "GYTuKRA4lKcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
        "\n",
        "## let's get counts for the first 5 tweets in the data\n",
        "example_train_vectors = count_vectorizer.fit_transform(train_data[\"Final_text\"])"
      ],
      "metadata": {
        "id": "R0avRU938hsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_train_vectors[0].todense().shape)\n",
        "print(example_train_vectors[0].todense())"
      ],
      "metadata": {
        "id": "s6zgxUkJ8pYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r1NXmdfatCzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TN3J8tj4tCvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Cleaning the text\n",
        "\n",
        "import string\n",
        "def text_process(text):\n",
        "    '''\n",
        "    Takes in a string of text, then performs the following:\n",
        "    1. Remove all punctuation\n",
        "    2. Remove all stopwords\n",
        "    3. Return the cleaned text as a list of words\n",
        "    4. Remove words\n",
        "    '''\n",
        "    stemmer = WordNetLemmatizer()\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
        "    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n",
        "    return [stemmer.lemmatize(word) for word in nopunc]"
      ],
      "metadata": {
        "id": "hiYWw9hYtCMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "data = train_data['Final_text']\n",
        "tfidfconvert = TfidfVectorizer(analyzer=text_process,ngram_range=(1,3)).fit(data)\n",
        "\n",
        "X_transformed=tfidfconvert.transform(data)\n",
        "\n",
        "# Clustering the training sentences with K-means technique\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "modelkmeans = KMeans(n_clusters=3, init='k-means++', n_init=100)\n",
        "modelkmeans.fit(X_transformed)"
      ],
      "metadata": {
        "id": "S_PvmGIqscVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "labels = modelkmeans.labels_\n",
        "print(accuracy_score(labels,train_data['target']))"
      ],
      "metadata": {
        "id": "bsO7zwVhtTM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "Sum_of_squared_distances = []\n",
        "K = range(1,15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(X_transformed)\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QpuI4ZMyxLGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "data = train_data['Final_text']\n",
        "\n",
        "\n",
        "tf_idf_vectorizor = TfidfVectorizer(stop_words = 'english',#tokenizer = tokenize_and_stem,\n",
        "                             max_features = 20000)\n",
        "tf_idf = tf_idf_vectorizor.fit_transform(data)\n",
        "tf_idf_norm = normalize(tf_idf)\n",
        "tf_idf_array = tf_idf_norm.toarray()"
      ],
      "metadata": {
        "id": "rZAQEr6eef71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectors = count_vectorizer.fit_transform(train_data[\"Final_text\"])\n",
        "\n",
        "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
        "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
        "# i.e. that the train and test vectors use the same set of tokens.\n",
        "# test_vectors = count_vectorizer.transform(test_df[\"text\"])"
      ],
      "metadata": {
        "id": "IuRsgfoZ9CjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}